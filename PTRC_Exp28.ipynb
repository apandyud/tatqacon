{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5c7600-0f0a-49a3-9958-ef7e591182d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import tatqa_utils\n",
    "import pandas as pd\n",
    "import table_convert\n",
    "import pyreadstat\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import utils\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "import numpy as np\n",
    "from progress.bar import Bar\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebe96b8-fbe7-4d44-a796-526d3885faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('dataset_raw/openai.api.key', 'r') as filek: \n",
    "    openai_key = filek.read()\n",
    "os.environ[\"OPENAI_API_KEY\"] =  openai_key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46380706-66bd-43ed-bfa2-fd9b3952a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.cache import SQLiteCache\n",
    "\n",
    "#llm = ChatOpenAI(temperature=0)\n",
    "#llm = OpenAI(temperature=0) \n",
    "#set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9196d9ce-8802-42d7-8c29-5f51ad5f5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "devdf = pd.read_json('dataset_raw/tatqa_dataset_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ea9f2293-e395-4f66-8dc9-f40e6dd3f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(qid):\n",
    "    for i, item in devdf.iterrows():\n",
    "        for q in item['questions']:        \n",
    "            if q['uid'] == qid:\n",
    "                #table = item['table']['table']\n",
    "                return (item['table'], q)\n",
    "    return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a29d3ef1-34c9-4f04-90bd-3b1c80bb8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_code3(llm, messages, question, value_list): \n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    chain = prompt | llm | output_parser\n",
    "    \n",
    "    response = chain.invoke({\"value_list\": value_list, \"question\":question})\n",
    "    code =  response.replace('```python','').replace('```','')\n",
    "    \n",
    "    return (\"\", code)\n",
    "\n",
    "def exec_code(code, value_list):  \n",
    "        try: \n",
    "            loc = locals()   \n",
    "            if not \"run()\" in code:\n",
    "                exec(code + f\"\\nr = run({value_list})\\n\", globals(), loc)\n",
    "            else: \n",
    "                exec(code + \"\\nr = run()\\n\", globals(), loc)\n",
    "            return loc['r']\n",
    "        except Exception as e:\n",
    "                s = '[Error]'+ str(e)\n",
    "                print(s)\n",
    "                return (s,'')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "270e5e76-8ffa-44b4-9fa0-a56d391092a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature = 0, top_p = 1.0, seed=11)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", \n",
    "temperature=0,\n",
    "    #max_tokens=150,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "cceb21d7-f08a-411f-b086-34253a608405",
   "metadata": {},
   "outputs": [],
   "source": [
    " messages=[\n",
    "         (\"system\",\"You are a helpful assistant with a subtask in a question-answering pipeline. The questions are related to a financial report. \" + \n",
    "             \"Financial report is stored as a list of annotated values. You will receive the financial report as an annotated value list and the question. \"+\n",
    "             \"Your task is to generate a Python function that can calculate a numeric value that is the answer for the received question. \"                            \n",
    "         ),\n",
    "        (\n",
    "          \"human\",\n",
    "          \"Here is the financial report as a list of annotated values: {value_list}\"\n",
    "        ),         \n",
    "        (\n",
    "          \"ai\",\n",
    "          \"Ok, I received the value list.\"\n",
    "        ),\n",
    "        (\n",
    "          \"human\",\n",
    "          \"Here is the question requires calculation on the financial report: {question}\"\n",
    "        ),\n",
    "        (\n",
    "          \"ai\",\n",
    "          \"Ok, I received the question.\"\n",
    "        ),\n",
    "        (\n",
    "          \"human\",\n",
    "          \"Generate a Python function 'run(value_list)' that can answer the question using the list of annotated values! \"+\n",
    "            \"The function must return a tuple (number, scale). The resulting number is a float with accuracy to two decimal places. Scale usually is thousand, million, billion, percent or an empty string. \"+\n",
    "            \"The calculation usually involves two steps: a selection and a calculation on selected data. \"+\n",
    "            #\"In the value list, each value represents one year. \"+\n",
    "            \"If the question is about calculating the average value in a given year, you must calculate the average between the given year and the previous one. It is aka Average Book Value. ex. 2012_average = (2012_value + 2011_value)/2  \"+\n",
    "            \"If the question is about calculating the change in average values in years, you must calculate the year average values (average between the given year and the previous one)  before difference calculation. ex. change_in _average_2011_2012 = (2012_value + 2011_value)/2 - (2011_value + 2010_value)/2 \"\n",
    "            \"The code can be specific to the provided value list. \" +\n",
    "            \"Do not generate explanation, nor example code, just the function.\"\n",
    "        ),\n",
    "        (\n",
    "          \"ai\",\n",
    "          \"Ok, I have all the information. The Python function is as follows:\"\n",
    "        ),\n",
    "      ]     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba26933b-bd20-48fc-8edb-d7892b87a9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "a859e835-5a7f-42ce-b823-f0937c10c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the change between 2018 and 2019 average free cash flow?\n",
      "\n",
      "def run(value_list):\n",
      "    free_cash_flow_2019 = 4411.0\n",
      "    free_cash_flow_2018 = 4044.0\n",
      "    average_free_cash_flow_2019 = (free_cash_flow_2019 + free_cash_flow_2018) / 2\n",
      "    free_cash_flow_2017 = 3316.0\n",
      "    average_free_cash_flow_2018 = (free_cash_flow_2018 + free_cash_flow_2017) / 2\n",
      "    change_average_free_cash_flow_2018_2019 = average_free_cash_flow_2019 - average_free_cash_flow_2018\n",
      "    return (round(change_average_free_cash_flow_2018_2019, 2), 'million')\n",
      "\n",
      "\u001b[92m Success: 547.5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "devdf = pd.read_json('dataset_raw/tatqa_dataset_dev.json')\n",
    "\n",
    "#qid = 'ba6783f3-8207-419a-b407-3f688682caef'\n",
    "#qid = 'a0414f81-8dc2-44b2-a441-2c9d9c805c4d'\n",
    "\n",
    "#qid = 'bf7abd62-d9cd-48d2-8826-1457684019a3'\n",
    "#qid = '4d259081-6da6-44bd-8830-e4de0031744c'\n",
    "#qid = 'dc5e217a-a7b3-4fc9-ac0f-13d328f26b20'\n",
    "\n",
    "#qid = '7cd3aedf-1291-4fea-bc9d-a25c65727b7b'\n",
    "qid = '22e20f25-669a-46b9-8779-2768ba391955'\n",
    "#qid = '65ec782c-691e-45df-b541-caecb85154ff'\n",
    "#qid = 'a983501d-2eec-486d-9661-e520c7c8af5e'\n",
    "\n",
    "\n",
    "_table, _q =  get_question(qid)\n",
    "table = _table['table']\n",
    "\n",
    "#[print(r) for r in table]\n",
    "\n",
    "q = _q['question']\n",
    "print(q)\n",
    "values = table_convert.convert_table(table)\n",
    "#response = chain.invoke({\"value_list\": values, \"question\":q})\n",
    "#code =  response.replace('```python','').replace('```','')\n",
    "p, code = gen_code3(llm, messages, q, values)\n",
    "print(code)\n",
    "(v, s) = exec_code(code, values)\n",
    "\n",
    "if v == _q['answer']:\n",
    "    print(\"\\033[92m Success: \" + str(v)+'\\033[0m')\n",
    "else:    \n",
    "    print(\"\\033[91m failure: \" + str(v), 'good answer: ', _q['answer'],'\\033[0m' )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914c0ec-d83b-4969-9364-24a749810a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "00088101-21e8-4cd6-a07f-a5128e5abe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the ratio of IMFTâ€™s total assets to total liabilities in 2019?\u001b[91m failure: 1.0 good answer:  2.93 \u001b[0m\n",
      "What is the 2019 average defined contribution schemes?\u001b[92m Success: 172.0\u001b[0m\n",
      "What is the 2019 average defined benefit schemes?\u001b[92m Success: 50.5\u001b[0m\n",
      "What is the difference between 2019 average defined contribution schemes and 2019 average defined benefit schemes?\u001b[91m failure: 138.33 good answer:  121.5 \u001b[0m\n",
      "What is the 2019 average free cash flow?\u001b[91m failure: 4411.0 good answer:  4227.5 \u001b[0m\n",
      "What is the 2018 average free cash flow?\u001b[91m failure: 3923.67 good answer:  3680 \u001b[0m\n",
      "What is the change between 2018 and 2019 average free cash flow?[Error]name 'free_cash_flow_2017' is not defined\n",
      "\u001b[92m Success: 547.5\u001b[0m\n",
      "What was the change in Capital redemption reserve in 2019 from 2018?\u001b[91m failure: 0.7 good answer:  0.2 \u001b[0m\n",
      "What was the average difference between EBITDA and underlying EBITDA for both FYs?\u001b[91m failure: -2349.0 good answer:  3728 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "devdf = pd.read_json('dataset_raw/tatqa_dataset_dev.json')\n",
    "\n",
    "qids = [\n",
    "'ba6783f3-8207-419a-b407-3f688682caef',\n",
    "'a0414f81-8dc2-44b2-a441-2c9d9c805c4d',\n",
    "'bf7abd62-d9cd-48d2-8826-1457684019a3',\n",
    "'4d259081-6da6-44bd-8830-e4de0031744c',\n",
    "'dc5e217a-a7b3-4fc9-ac0f-13d328f26b20',\n",
    "\n",
    "'7cd3aedf-1291-4fea-bc9d-a25c65727b7b',\n",
    "'22e20f25-669a-46b9-8779-2768ba391955',\n",
    "'65ec782c-691e-45df-b541-caecb85154ff',\n",
    "'a983501d-2eec-486d-9661-e520c7c8af5e'\n",
    "]\n",
    "def get_answer(llm, messages, table, q):\n",
    "    values = table_convert.convert_table(table)    \n",
    "    p, code = gen_code3(llm, messages, q, values)    \n",
    "    #print(code)\n",
    "    (v, s) = exec_code(code, values)\n",
    "    return  (v, s)\n",
    "\n",
    "    \n",
    "\n",
    "for qid in qids:\n",
    "    _table, _q =  get_question(qid)\n",
    "    table = _table['table']\n",
    "    \n",
    "    #[print(r) for r in table]\n",
    "    \n",
    "    q = _q['question']\n",
    "    print(q, end='')\n",
    "    \n",
    "    r = []\n",
    "    for i in [0,1,2]:        \n",
    "       r.append(get_answer(llm, messages, table, q))\n",
    "    \n",
    "    if r[0][0] == r[1][0]:\n",
    "        v = r[0][0]\n",
    "    elif r[1][0] == r[2][0]:\n",
    "        v = r[1][0]\n",
    "    elif r[0][0] == r[2][0]:\n",
    "        v = r[0][0] \n",
    "    else:\n",
    "        v = None\n",
    "        \n",
    "    #(v, s) = get_answer(llm, messages, table, q)    \n",
    "    \n",
    "    if v == _q['answer']:\n",
    "        print(\"\\033[92m Success: \" + str(v)+'\\033[0m')\n",
    "    else:    \n",
    "        print(\"\\033[91m failure: \" + str(v), 'good answer: ', _q['answer'],'\\033[0m' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb542a14-a138-46d9-b97f-fdb2106f3846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23949634-5d64-480e-b71d-9c647c844409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd4185-4f3c-4a08-b283-ce1b246abe85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:arpad_tab]",
   "language": "python",
   "name": "conda-env-arpad_tab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
